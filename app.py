# app.py â€” Tesla One-Stop (FREE) + YouTube ë¼ì´ë¸Œ/ìµœì‹  ì˜ìƒ
import os
import time
import re
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urlparse, parse_qs

import pandas as pd
import requests
import yfinance as yf
import feedparser
import plotly.graph_objects as go
import streamlit as st
from xml.etree import ElementTree as ET

st.set_page_config(page_title="Tesla One-Stop (Free)", page_icon="ğŸš—", layout="wide")

# ---------------------------
# ê³µí†µ ì„¤ì •/ìƒìˆ˜
# ---------------------------
TSLA = "TSLA"
CUSIP_TSLA_PREFIX = "88160R"
INTRADAY = {"1m","2m","5m","15m","30m","60m","90m"}

# ê¸°ë³¸ ê¸°ê´€ CIK (13F)
DEFAULT_CIKS = {
    "BlackRock Inc.": "0001364742",
    "The Vanguard Group, Inc.": "0000102909",
    "FMR LLC (Fidelity)": "0000315066",
    "State Street Corp.": "0000093751",
    "ARK Investment Management LLC": "0001697747",
    "T. Rowe Price Associates, Inc.": "0000080255",
}

# ë‰´ìŠ¤ RSS
RSS_SOURCES = {
    "Tesla IR (Press)": "https://ir.tesla.com/press-releases/rss",
    "Electrek â€“ Tesla": "https://electrek.co/guides/tesla/feed/",
    "Reuters â€“ Tesla": "https://feeds.reuters.com/reuters/businessNews?query=tesla",
    "CNBC â€“ Tesla": "https://www.cnbc.com/id/15839135/device/rss/rss.html?query=tesla",
    "Google News â€“ Tesla": "https://news.google.com/rss/search?q=Tesla%20OR%20TSLA&hl=en-US&gl=US&ceid=US:en",
}

# X(íŠ¸ìœ„í„°) ê³„ì •
X_USERNAMES = {
    "Elon Musk": "elonmusk",
    "Donald Trump": "realDonaldTrump",
    "Tesla": "Tesla",
}
X_USER_IDS = {  # í˜¸ì¶œ ì ˆì•½ìš© í•˜ë“œì½”ë”©
    "elonmusk": "44196397",
    "realDonaldTrump": "25073877",
    "Tesla": "13298072",
}

# ìœ íŠœë¸Œ ì±„ë„ (ì±„ë„IDëŠ” UIì—ì„œ ìˆ˜ì • ê°€ëŠ¥; ì—¬ê¸° ê¸°ë³¸ ì˜ˆì‹œ/ìë¦¬í‘œì‹œ)
DEFAULT_YT_CHANNELS = [
    {"ì±„ë„ëª…": "ì˜¤ëŠ˜ì˜ í…ŒìŠ¬ë¼ ë‰´ìŠ¤", "channel_id": "UCXq7NNALDnqafn3KFvIyJKA"},
    {"ì±„ë„ëª…": "ë§ˆí”¼ë”” ë¯¸êµ­ì£¼ì‹", "channel_id": "UCjp7GHSUKx9Joji3tIz1jCg"},
    {"ì±„ë„ëª…": "ì—”ì§€ë‹ˆì–´TV", "channel_id": "UCnvCYORRLMYMQNhrlBILdCg"},
    {"ì±„ë„ëª…": "ì˜¬ëœë„ í‚´ ë¯¸êµ­ì£¼ì‹", "channel_id": "UCwSSqi-s0wcH6pJbH3YPZqQ"},
]

# SEC
SEC_BASE = "https://data.sec.gov"
SEC_UA = {"User-Agent": "TeslaDash/1.0 (your-email@example.com)"}  # ë³¸ì¸ ì´ë©”ì¼ë¡œ êµì²´ ê¶Œì¥

# ì•ˆì „í•œ ì‹œí¬ë¦¿ ì ‘ê·¼ (secrets.toml ì—†ì„ ë•Œë„ ì•ˆì „)
def get_secret(key: str, default: str = "") -> str:
    try:
        return st.secrets.get(key, default)
    except Exception:
        return os.getenv(key, default) or default

X_BEARER = get_secret("X_BEARER_TOKEN", "")
YOUTUBE_API_KEY = get_secret("YOUTUBE_API_KEY", "")

# ---------------------------
# í˜ì´ì§€ ìë™ ìƒˆë¡œê³ ì¹¨(JS, ì¶”ê°€ ì˜ì¡´ì„± ç„¡)
# ---------------------------
def auto_refresh_html(seconds: int, key: str):
    ms = int(seconds * 1000)
    st.components.v1.html(
        f"""
        <script>
        (function(){{
            const KEY="{key}";
            if(!window.__autoRefreshTimers) window.__autoRefreshTimers = {{}};
            if(!window.__autoRefreshTimers[KEY]) {{
                window.__autoRefreshTimers[KEY] = setTimeout(function(){{
                    const url = new URL(window.location.href);
                    url.searchParams.set(KEY, Date.now().toString());
                    window.location.href = url.toString();
                }}, {ms});
            }}
        }})();
        </script>
        """, height=0
    )

# ---------------------------
# ê°€ê²©/ì°¨íŠ¸ (Yahoo + Stooq fallback)
# ---------------------------
@st.cache_data(ttl=120)
def safe_yf_download(symbol: str, period: str, interval: str):
    """
    ê°•ì¸í•œ ê°€ê²© ìˆ˜ì§‘:
    1) yfinance.Ticker().history (ê¶Œì¥)
    2) yfinance.download (threads=False)
    3) interval/period ìë™ ë³´ì •
    4) Stooq ì¼ë´‰ ìµœì¢… ë°±ì—…
    """
    import requests, pandas as pd, yfinance as yf
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})

    # ğŸ”§ ê°„ê²© ë³´ì •: ì¼ë¶€ í™˜ê²½ì—ì„œ '1h' ëŒ€ì‹  '60m'ê°€ ì•ˆì •ì 
    interval_fixed = {"1h": "60m"}.get(interval, interval)

    # ìš”ì²­ í›„ë³´(ìš°ì„ ìˆœìœ„)
    combos = [(period, interval_fixed)]
    if interval_fixed == "1m" and period not in {"1d","5d","7d"}:
        combos.append(("5d","1m"))
    if interval_fixed in INTRADAY and period not in {"1d","5d","7d","1mo","3mo","6mo","60d","90d"}:
        combos.append(("1mo","5m"))
    combos.append(("1y","1d"))  # ìµœí›„ì˜ ì•¼í›„ ë°ì¼ë¦¬

    last_err = None

    # 1) Ticker().history (ê°€ì¥ ì•ˆì •)
    try:
        tkr = yf.Ticker(symbol, session=session)
        df = tkr.history(period=period, interval=interval_fixed, prepost=False, auto_adjust=False)
        if not df.empty:
            return df, (period, interval_fixed), "yfinance.history"
    except Exception as e:
        last_err = f"history: {e}"

    # 2) download ì¬ì‹œë„ (threads=Falseê°€ ì˜¤ë¥˜ ì¤„ì„)
    for p, i in combos:
        try:
            df = yf.download(
                symbol, period=p, interval=i,
                auto_adjust=False, prepost=False,
                progress=False, threads=False, session=session
            )
            if isinstance(df.columns, pd.MultiIndex):
                df.columns = [c[0] for c in df.columns]
            if not df.empty:
                return df, (p, i), "yfinance.download"
        except Exception as e:
            last_err = f"download {p}/{i}: {e}"

    # 3) Stooq ì¼ë´‰ ë°±ì—…
    try:
        import pandas as pd
        stooq = pd.read_csv("https://stooq.com/q/d/l/?s=tsla.us&i=d")
        stooq.rename(columns={"Date":"Datetime"}, inplace=True)
        stooq["Datetime"] = pd.to_datetime(stooq["Datetime"])
        stooq.set_index("Datetime", inplace=True)
        stooq = stooq.dropna()
        if not stooq.empty:
            return stooq, ("stooq-daily","1d"), "Yahoo empty â†’ Stooq daily fallback"
    except Exception as e:
        last_err = f"Stooq: {e}"

    # ëª¨ë‘ ì‹¤íŒ¨
    return pd.DataFrame(), None, last_err

def plot_candles(df: pd.DataFrame, title: str):
    if df.empty:
        st.error("ì°¨íŠ¸ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return
    fig = go.Figure([go.Candlestick(
        x=df.index, open=df["Open"], high=df["High"], low=df["Low"], close=df["Close"], name="Price"
    )])
    if "Volume" in df.columns:
        fig.add_trace(go.Bar(x=df.index, y=df["Volume"], name="Volume", yaxis="y2", opacity=0.3))
        fig.update_layout(
            yaxis=dict(domain=[0.3,1.0], title="Price"),
            yaxis2=dict(domain=[0.0,0.25], title="Volume"),
            xaxis=dict(title="Time"),
            title=title, height=600, margin=dict(l=10, r=10, t=40, b=10)
        )
    else:
        fig.update_layout(title=title, height=550, margin=dict(l=10, r=10, t=40, b=10))
    st.plotly_chart(fig, use_container_width=True)

# ---------------------------
# ë‰´ìŠ¤ RSS
# ---------------------------
@st.cache_data(ttl=300)
def fetch_rss(feed_url: str, limit: int = 12) -> List[Dict]:
    parsed = feedparser.parse(feed_url)
    items = []
    for e in parsed.entries[:limit]:
        items.append({
            "title": e.get("title"),
            "link": e.get("link"),
            "published": e.get("published", ""),
            "summary": re.sub("<.*?>","", e.get("summary","")) if e.get("summary") else "",
        })
    return items

# ---------------------------
# SEC 13F (ë¬´ë£Œ)
# ---------------------------
def _strip_xml_ns(xml_text: str) -> str:
    return re.sub(r'\sxmlns(:\w+)?="[^"]+"','', xml_text)

def _to_int(x):
    try:
        return int(str(x).replace(",","").strip())
    except Exception:
        return None

@st.cache_data(ttl=3600)
def sec_recent_filings(cik: str) -> pd.DataFrame:
    cik10 = str(cik).zfill(10)
    url = f"{SEC_BASE}/submissions/CIK{cik10}.json"
    r = requests.get(url, headers=SEC_UA, timeout=20)
    r.raise_for_status()
    js = r.json()
    rec = js.get("filings", {}).get("recent", {})
    return pd.DataFrame(rec)

def _acc_nodash(acc: str) -> str:
    return acc.replace("-","")

@st.cache_data(ttl=3600)
def sec_list_13f_accessions(cik: str, limit=3) -> List[Dict]:
    df = sec_recent_filings(cik)
    if df.empty: return []
    mask = df["form"].isin(["13F-HR","13F-HR/A"])
    sdf = df[mask].head(limit)
    rows = []
    for _, r in sdf.iterrows():
        rows.append({
            "cik": cik,
            "accession": r["accessionNumber"],
            "reportDate": r.get("reportDate",""),
            "primaryDocument": r.get("primaryDocument",""),
        })
    return rows

@st.cache_data(ttl=3600)
def sec_find_infotable_url(cik: str, accession: str) -> Optional[str]:
    cik_nozero = str(int(cik)); acc = _acc_nodash(accession)
    idx = f"https://www.sec.gov/Archives/edgar/data/{cik_nozero}/{acc}/index.json"
    r = requests.get(idx, headers=SEC_UA, timeout=20)
    if not r.ok: return None
    files = r.json().get("directory",{}).get("item",[])
    for f in files:
        name = f.get("name","").lower()
        if name.endswith(".xml") and ("infotable" in name or "informationtable" in name or "form13f" in name):
            return f"https://www.sec.gov/Archives/edgar/data/{cik_nozero}/{acc}/{f['name']}"
    for f in files:
        name = f.get("name","").lower()
        if name.endswith(".txt"):
            return f"https://www.sec.gov/Archives/edgar/data/{cik_nozero}/{acc}/{f['name']}"
    return None

def _parse_infotable_xml(xml_text: str) -> pd.DataFrame:
    m = re.search(r"<informationTable[\s\S]*</informationTable>", xml_text, re.IGNORECASE)
    if m: xml_text = m.group(0)
    xml_text = _strip_xml_ns(xml_text)
    root = ET.fromstring(xml_text.encode("utf-8"))
    rows = []
    for it in root.iterfind(".//infoTable"):
        issuer = (it.findtext("nameOfIssuer","") or "").strip()
        cusip = (it.findtext("cusip","") or "").strip()
        amt = it.find(".//shrsOrPrnAmt/sshPrnamt")
        shares = _to_int(amt.text) if (amt is not None and amt.text) else None
        val = _to_int(it.findtext("value"))
        value_usd = val*1000 if val is not None else None
        rows.append({"issuer":issuer,"cusip":cusip,"shares":shares,"value_usd":value_usd})
    return pd.DataFrame(rows)

@st.cache_data(ttl=3600)
def sec_tsla_position_from_13f(cik: str, accession: str) -> Optional[Dict]:
    url = sec_find_infotable_url(cik, accession)
    if not url: return None
    r = requests.get(url, headers=SEC_UA, timeout=30)
    if not r.ok or not r.text: return None
    try:
        df = _parse_infotable_xml(r.text)
    except Exception:
        return None
    if df.empty:
        return {"shares":0, "value_usd":0}
    m = df[(df["cusip"].str.startswith(CUSIP_TSLA_PREFIX, na=False)) |
           (df["issuer"].str.contains("TESLA", case=False, na=False))]
    if m.empty:
        return {"shares":0, "value_usd":0}
    return {
        "shares": int(m["shares"].fillna(0).sum()),
        "value_usd": int(m["value_usd"].fillna(0).sum())
    }

def build_13f_table(managers: Dict[str, str]) -> pd.DataFrame:
    out = []
    for name, cik in managers.items():
        try:
            accs = sec_list_13f_accessions(cik, limit=2)
            if not accs: continue
            latest = accs[0]; prev = accs[1] if len(accs)>1 else None
            latest_pos = sec_tsla_position_from_13f(cik, latest["accession"])
            time.sleep(0.4)
            prev_pos = sec_tsla_position_from_13f(cik, prev["accession"]) if prev else None
            shares = latest_pos["shares"] if latest_pos else None
            value_usd = latest_pos["value_usd"] if latest_pos else None
            delta = None
            if latest_pos and prev_pos:
                delta = (latest_pos["shares"] or 0) - (prev_pos["shares"] or 0)
            out.append({
                "ê¸°ê´€/í€ë“œ": name,
                "CIK": cik,
                "ë³´ê³ ì¼(ìµœê·¼)": latest.get("reportDate",""),
                "ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)": shares,
                "í‰ê°€ì•¡(USD, ìµœê·¼)": value_usd,
                "ë³´ìœ ì£¼ìˆ˜ ì¦ê°(qoq)": delta
            })
        except Exception:
            continue
    df = pd.DataFrame(out)
    if not df.empty:
        df = df.sort_values(by=["ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)"], ascending=False)
    return df

# ---------------------------
# X API (ì„ íƒ, ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ êµ¬ì¡° ìœ ì§€)
# ---------------------------
def _x_headers():
    if not X_BEARER: return None
    return {"Authorization": f"Bearer {X_BEARER}"}

def _x_api_get(url, params=None, timeout=15):
    h = _x_headers()
    if not h: return None
    for base in ("https://api.x.com", "https://api.twitter.com"):
        try:
            r = requests.get(base+url, headers=h, params=params, timeout=timeout)
            if r.ok: return r.json()
        except Exception:
            pass
    return None

@st.cache_data(ttl=24*3600)
def x_get_user_id(username: str) -> Optional[str]:
    if username in X_USER_IDS: return X_USER_IDS[username]
    js = _x_api_get(f"/2/users/by/username/{username}")
    if not js: return None
    return js.get("data",{}).get("id")

@st.cache_data(ttl=0)
def x_fetch_latest_tweets(user_id: str, since_id: Optional[str]=None, max_results: int=5):
    params = {"max_results":str(max_results), "exclude":"retweets,replies",
              "tweet.fields":"created_at,public_metrics,entities"}
    if since_id: params["since_id"] = since_id
    js = _x_api_get(f"/2/users/{user_id}/tweets", params=params)
    if not js: return [], since_id
    data = js.get("data", [])
    data.sort(key=lambda t: t.get("id"), reverse=False)
    new_since = data[-1]["id"] if data else since_id
    return data, new_since

def _format_tweet_text(t):
    txt = t.get("text","")
    ents = (t.get("entities") or {}).get("urls", []) or []
    for url in ents:
        u = url.get("url"); ex = url.get("expanded_url") or u
        if u: txt = txt.replace(u, ex)
    return txt

# ---------------------------
# YouTube (RSS + Data API v3)
# ---------------------------
@st.cache_data(ttl=300)
def yt_rss_latest(channel_id: str, limit: int = 6):
    """API í‚¤ ì—†ì´ ìµœì‹  ì˜ìƒ ë¦¬ìŠ¤íŠ¸"""
    feed = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    parsed = feedparser.parse(feed)
    items = []
    for e in parsed.entries[:limit]:
        vid = e.get("yt_videoid")
        if not vid:
            # ë§í¬ì—ì„œ v íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            try:
                q = parse_qs(urlparse(e.get("link","")).query)
                vid = q.get("v",[None])[0]
            except Exception:
                vid = None
        items.append({
            "video_id": vid,
            "title": e.get("title",""),
            "link": e.get("link",""),
            "published": e.get("published",""),
        })
    return items

@st.cache_data(ttl=60)
def yt_api_live_videos(channel_id: str, max_results: int = 3):
    """API í‚¤ê°€ ìˆì„ ë•Œ í•´ë‹¹ ì±„ë„ì˜ ë¼ì´ë¸Œ ì¤‘ì¸ ì˜ìƒ ê²€ìƒ‰"""
    if not YOUTUBE_API_KEY:
        return []
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "part": "snippet",
        "channelId": channel_id,
        "eventType": "live",
        "type": "video",
        "order": "date",
        "maxResults": str(max_results),
        "key": YOUTUBE_API_KEY,
    }
    r = requests.get(url, params=params, timeout=15)
    if not r.ok:
        return []
    data = r.json().get("items", [])
    out = []
    for it in data:
        vid = it.get("id",{}).get("videoId")
        sn = it.get("snippet",{})
        out.append({
            "video_id": vid,
            "title": sn.get("title","(live)"),
            "published": sn.get("publishedAt",""),
            "link": f"https://www.youtube.com/watch?v={vid}" if vid else "",
        })
    return out

@st.cache_data(ttl=300)
def yt_api_latest_videos(channel_id: str, max_results: int = 6):
    """API í‚¤ê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ APIë¡œ ìµœì‹  ì˜ìƒ(ì—…ë¡œë“œ) ì¡°íšŒ; ì—†ìœ¼ë©´ RSSë¥¼ ì“°ëŠ” ê²Œ ë‚«ë‹¤."""
    if not YOUTUBE_API_KEY:
        return yt_rss_latest(channel_id, max_results)
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "part": "snippet",
        "channelId": channel_id,
        "type": "video",
        "order": "date",
        "maxResults": str(max_results),
        "key": YOUTUBE_API_KEY,
    }
    r = requests.get(url, params=params, timeout=15)
    if not r.ok:
        return yt_rss_latest(channel_id, max_results)
    data = r.json().get("items", [])
    out = []
    for it in data:
        vid = it.get("id",{}).get("videoId")
        sn = it.get("snippet",{})
        out.append({
            "video_id": vid,
            "title": sn.get("title",""),
            "published": sn.get("publishedAt",""),
            "link": f"https://www.youtube.com/watch?v={vid}" if vid else "",
        })
    return out

def yt_embed(video_id: str, height: int = 315):
    if not video_id:
        return
    st.components.v1.html(
        f"""
        <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;">
          <iframe src="https://www.youtube.com/embed/{video_id}"
                  title="YouTube video" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
        </div>
        """,
        height=height+60
    )

# ---------------------------
# UI
# ---------------------------
st.sidebar.title("Tesla One-Stop (Free)")
page = st.sidebar.radio(
    "ë©”ë‰´",
    ["ğŸ“ˆ ì°¨íŠ¸", "ğŸ“° ë‰´ìŠ¤/ì½”ë©˜íŠ¸", "ğŸ“º ìœ íŠœë¸Œ", "ğŸ¦ ì§€ë¶„ ë³€ë™(13F, ë¬´ë£Œ)", "âš™ï¸ ì˜µì…˜/ì„¤ì •"]
)

# ---- ì°¨íŠ¸ ----
if page == "ğŸ“ˆ ì°¨íŠ¸":
    st.title("ğŸ“ˆ TSLA ì°¨íŠ¸ (ì•ˆì •í™” ë²„ì „)")
    c1,c2,c3 = st.columns(3)
    with c1:
        period = st.selectbox("ê¸°ê°„", ["1d","5d","7d","1mo","3mo","6mo","1y"], index=0)
    with c2:
        interval = st.selectbox("ë´‰ ê°„ê²©", ["1m","5m","15m","1h","1d"], index=0)
    with c3:
        if st.button("ìºì‹œ ì´ˆê¸°í™”"):
            st.cache_data.clear()
            st.success("ìºì‹œ ì‚­ì œ ì™„ë£Œ. ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦")
    df, used, note = safe_yf_download("TSLA", period, interval)
    if note: st.caption(f"ì†ŒìŠ¤: {note}")
    if df.empty:
        st.error("ì‹œì„¸ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”.")
        with st.expander("ë„¤íŠ¸ì›Œí¬ ì§„ë‹¨"):
            import requests
            tests = [
                "https://query2.finance.yahoo.com/v1/finance/trending/US?count=1",
                "https://query2.finance.yahoo.com/v8/finance/chart/TSLA?range=1d&interval=1m",
                "https://stooq.com/q/d/l/?s=tsla.us&i=d",
            ]
            for u in tests:
                try:
                    r = requests.get(u, headers={"User-Agent":"Mozilla/5.0"}, timeout=6)
                    st.write(u, "â†’", r.status_code, f"{len(r.content)} bytes")
                except Exception as e:
                    st.write(u, "â†’", str(e))
    else:
        plot_candles(df, f"TSLA {used[0]}/{used[1]}")
# ---- ë‰´ìŠ¤/X ----
elif page == "ğŸ“° ë‰´ìŠ¤/ì½”ë©˜íŠ¸":
    st.title("ğŸ“° ë‰´ìŠ¤ & ì½”ë©˜íŠ¸")
    t1,t2 = st.tabs(["ë‰´ìŠ¤ RSS","ìœ ëª…ì¸ ì½”ë©˜íŠ¸ (X ì„ë² ë“œ ë¦¬í”„ë ˆì‹œ)"])
    with t1:
        cols = st.columns(2)
        keys = list(RSS_SOURCES.keys())
        left, right = keys[:(len(keys)+1)//2], keys[(len(keys)+1)//2:]
        for col, group in zip(cols, [left, right]):
            with col:
                for k in group:
                    st.subheader(k)
                    for it in fetch_rss(RSS_SOURCES[k], limit=7):
                        st.markdown(f"- **[{it['title']}]({it['link']})**")
                        if it["published"]: st.caption(it["published"])
                    st.markdown("---")
    with t2:
        acct_label = st.selectbox("ê³„ì • ì„ íƒ", list(X_USERNAMES.keys()))
        handle = X_USERNAMES[acct_label]
        refresh_sec = st.slider("ìƒˆë¡œê³ ì¹¨(ì´ˆ)", 15, 180, 60, step=15)
        auto_refresh_html(refresh_sec, key=f"x_refresh_{handle}")
        embed_html = f"""
        <a class="twitter-timeline" href="https://twitter.com/{handle}?ref_src=twsrc%5Etfw">
          Tweets by @{handle}
        </a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        """
        st.components.v1.html(embed_html, height=800, scrolling=True)

# ---- ìœ íŠœë¸Œ ----
elif page == "ğŸ“º ìœ íŠœë¸Œ":
    st.title("ğŸ“º í…ŒìŠ¬ë¼ ìœ íŠœë²„ â€” ë¼ì´ë¸Œ/ìµœì‹  ì˜ìƒ")

    # 1) ì±„ë„ ëª©ë¡ í¸ì§‘
    st.subheader("ì±„ë„ ëª©ë¡ í¸ì§‘")
    df_channels = st.session_state.get("yt_channels_df") or pd.DataFrame(DEFAULT_YT_CHANNELS)
    df_channels = st.data_editor(df_channels, num_rows="dynamic", key="yt_channels_editor")
    st.session_state["yt_channels_df"] = df_channels

    st.markdown("---")

    # 2) ë¼ì´ë¸Œ ì²´í¬(ì„ íƒ)
    st.subheader("ì§€ê¸ˆ ë¼ì´ë¸Œ ì¤‘ ğŸ”´")
    refresh_live = st.checkbox("ìë™ ìƒˆë¡œê³ ì¹¨(ì´ˆ) ì„¤ì •", value=True)
    live_interval = st.slider("ë¼ì´ë¸Œ ì²´í¬ ì£¼ê¸°(ì´ˆ)", 30, 180, 60, step=15, disabled=not refresh_live)
    if refresh_live:
        auto_refresh_html(live_interval, key="yt_live_refresh")

    if df_channels.empty:
        st.info("ì±„ë„ì„ ì¶”ê°€í•˜ì„¸ìš”. ì˜ˆ: channel_id = UC_x5XG1OV2P6uZZ5FSM9TtQ")
    else:
        if not YOUTUBE_API_KEY:
            st.info("YOUTUBE_API_KEYê°€ ì—†ì–´ì„œ ë¼ì´ë¸Œ ìƒíƒœëŠ” API ì—†ì´ í™•ì¸í•©ë‹ˆë‹¤. ê° ì±„ë„ì˜ `/live` ë§í¬ë¥¼ ëˆŒëŸ¬ í™•ì¸í•˜ì„¸ìš”.")
        live_cols = st.columns(3)
        idx = 0
        for _, row in df_channels.iterrows():
            name = str(row.get("ì±„ë„ëª…","")).strip()
            cid  = str(row.get("channel_id","")).strip()
            if not cid: continue
            # APIê°€ ìˆìœ¼ë©´ ë¼ì´ë¸Œ ê²€ìƒ‰
            lives = yt_api_live_videos(cid, max_results=2) if YOUTUBE_API_KEY else []
            with live_cols[idx % 3]:
                if lives:
                    for lv in lives:
                        st.markdown(f"**{name}** â€” ğŸ”´ LIVE: [{lv['title']}]({lv['link']})")
                        yt_embed(lv["video_id"])
                else:
                    st.markdown(f"**{name}** â€” í˜„ì¬ ë¼ì´ë¸Œ ê°ì§€ ì—†ìŒ")
                    st.caption(f"[ì±„ë„ ë¼ì´ë¸Œ í˜ì´ì§€ ë°”ë¡œê°€ê¸°](https://www.youtube.com/channel/{cid}/live)")
            idx += 1

    st.markdown("---")

    # 3) ìµœì‹  ì—…ë¡œë“œ
    st.subheader("ìµœì‹  ì—…ë¡œë“œ")
    per_channel = st.slider("ì±„ë„ë³„ í‘œì‹œ ê°œìˆ˜", 1, 8, 3)
    refresh_latest = st.checkbox("ìµœì‹  ì˜ìƒ ìë™ ìƒˆë¡œê³ ì¹¨", value=False)
    if refresh_latest:
        auto_refresh_html(90, key="yt_latest_refresh")

    if not df_channels.empty:
        for _, row in df_channels.iterrows():
            name = str(row.get("ì±„ë„ëª…","")).strip()
            cid  = str(row.get("channel_id","")).strip()
            if not cid: continue
            st.markdown(f"### {name}")
            vids = yt_api_latest_videos(cid, max_results=per_channel)
            if not vids:
                st.write("ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue
            cards = st.columns(len(vids))
            for col, v in zip(cards, vids):
                with col:
                    st.markdown(f"**[{v['title']}]({v['link']})**")
                    if v["video_id"]:
                        yt_embed(v["video_id"], height=200)
                    if v.get("published"):
                        st.caption(v["published"])
            st.markdown("---")

    st.caption("íŒ: ì±„ë„ IDëŠ” ì±„ë„ í˜ì´ì§€ URLì—ì„œ `channel/` ë’¤ 24ì IDì…ë‹ˆë‹¤. ì»¤ìŠ¤í…€ í•¸ë“¤ì´ë©´ 'ì±„ë„ ì •ë³´ > ê³µìœ  > ì±„ë„ ë§í¬'ë¡œ ì‹¤ì œ ID í™•ì¸.")

# ---- 13F (ë¬´ë£Œ)
elif page == "ğŸ¦ ì§€ë¶„ ë³€ë™(13F, ë¬´ë£Œ)":
    st.title("ğŸ¦ ê¸°ê´€ ë³´ìœ /ë³€ë™ â€” SEC 13F (ì™„ì „ ë¬´ë£Œ)")
    with st.expander("ëŒ€ìƒ ê¸°ê´€(í¸ì§‘ ê°€ëŠ¥)"):
        edit_df = pd.DataFrame([{"ê¸°ê´€/í€ë“œ": k, "CIK": v} for k,v in DEFAULT_CIKS.items()])
        edited = st.data_editor(edit_df, num_rows="dynamic", key="mgr_edit")
        managers = {row["ê¸°ê´€/í€ë“œ"]: str(row["CIK"]) for _, row in edited.iterrows()
                    if row.get("ê¸°ê´€/í€ë“œ") and row.get("CIK")}
    if st.button("13F ìƒˆë¡œ ê³ ì¹¨", type="primary"):
        st.cache_data.clear()
    with st.spinner("SECì—ì„œ 13F ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        df = build_13f_table(managers)
    if df.empty:
        st.warning("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        fmt = df.copy()
        if "ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)" in fmt:
            fmt["ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)"] = fmt["ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)"].map(lambda x: f"{x:,}" if pd.notna(x) else "")
        if "í‰ê°€ì•¡(USD, ìµœê·¼)" in fmt:
            fmt["í‰ê°€ì•¡(USD, ìµœê·¼)"] = fmt["í‰ê°€ì•¡(USD, ìµœê·¼)"].map(lambda x: f"${x:,}" if pd.notna(x) else "")
        if "ë³´ìœ ì£¼ìˆ˜ ì¦ê°(qoq)" in fmt:
            fmt["ë³´ìœ ì£¼ìˆ˜ ì¦ê°(qoq)"] = fmt["ë³´ìœ ì£¼ìˆ˜ ì¦ê°(qoq)"].map(lambda x: f"{x:+,}" if pd.notna(x) else "")
        st.dataframe(fmt, use_container_width=True)
    st.info("ì°¸ê³ : 13FëŠ” ë¶„ê¸° ë‹¨ìœ„ ê³µê°œì´ë©°, ì¼ì¤‘ ë³€ë™ì€ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ---- ì„¤ì •
elif page == "âš™ï¸ ì˜µì…˜/ì„¤ì •":
    st.title("âš™ï¸ ì˜µì…˜/ì„¤ì •")
    st.markdown("""
    - ìœ íŠœë¸Œ: **API í‚¤ ì—†ì´ë„** ìµœì‹  ì˜ìƒì€ RSSë¡œ í‘œì‹œë©ë‹ˆë‹¤. ë¼ì´ë¸Œ ê°ì§€ëŠ” **YOUTUBE_API_KEY**ê°€ ìˆì„ ë•Œ ìë™í™”ë©ë‹ˆë‹¤.
    - SEC: User-Agentì— **ì—°ë½ ê°€ëŠ¥í•œ ì´ë©”ì¼** í‘œê¸°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    - X API í´ë§ì„ ì“°ë©´ X_BEARER_TOKENì´ í•„ìš”í•©ë‹ˆë‹¤(ì—†ì–´ë„ ì„ë² ë“œëŠ” ë™ì‘).
    """)
