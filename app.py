# app.py ‚Äî Tesla One-Stop (FREE) + YouTube ÎùºÏù¥Î∏å/ÏµúÏã† ÏòÅÏÉÅ
import os
import time
import re
from datetime import datetime
from typing import List, Dict, Optional
from urllib.parse import urlparse, parse_qs

import pandas as pd
import requests
import yfinance as yf
import feedparser
import plotly.graph_objects as go
import streamlit as st
from xml.etree import ElementTree as ET

st.set_page_config(page_title="Tesla One-Stop (Free)", page_icon="üöó", layout="wide")

# ---------------------------
# Í≥µÌÜµ ÏÑ§Ï†ï/ÏÉÅÏàò
# ---------------------------
TSLA = "TSLA"
CUSIP_TSLA_PREFIX = "88160R"
INTRADAY = {"1m","2m","5m","15m","30m","60m","90m"}

# Í∏∞Î≥∏ Í∏∞Í¥Ä CIK (13F)
DEFAULT_CIKS = {
    "BlackRock Inc.": "0001364742",
    "The Vanguard Group, Inc.": "0000102909",
    "FMR LLC (Fidelity)": "0000315066",
    "State Street Corp.": "0000093751",
    "ARK Investment Management LLC": "0001697747",
    "T. Rowe Price Associates, Inc.": "0000080255",
}

# Îâ¥Ïä§ RSS
RSS_SOURCES = {
    "Tesla IR (Press)": "https://ir.tesla.com/press-releases/rss",
    "Electrek ‚Äì Tesla": "https://electrek.co/guides/tesla/feed/",
    "Reuters ‚Äì Tesla": "https://feeds.reuters.com/reuters/businessNews?query=tesla",
    "CNBC ‚Äì Tesla": "https://www.cnbc.com/id/15839135/device/rss/rss.html?query=tesla",
    "Google News ‚Äì Tesla": "https://news.google.com/rss/search?q=Tesla%20OR%20TSLA&hl=en-US&gl=US&ceid=US:en",
}

# X(Ìä∏ÏúÑÌÑ∞) Í≥ÑÏ†ï
X_USERNAMES = {
    "Elon Musk": "elonmusk",
    "Donald Trump": "realDonaldTrump",
    "Tesla": "Tesla",
}
X_USER_IDS = {  # Ìò∏Ï∂ú Ï†àÏïΩÏö© ÌïòÎìúÏΩîÎî©
    "elonmusk": "44196397",
    "realDonaldTrump": "25073877",
    "Tesla": "13298072",
}

# Ïú†ÌäúÎ∏å Ï±ÑÎÑê (Ï±ÑÎÑêIDÎäî UIÏóêÏÑú ÏàòÏ†ï Í∞ÄÎä•; Ïó¨Í∏∞ Í∏∞Î≥∏ ÏòàÏãú/ÏûêÎ¶¨ÌëúÏãú)
DEFAULT_YT_CHANNELS = [
    {"Ï±ÑÎÑêÎ™Ö": "Ïò§ÎäòÏùò ÌÖåÏä¨Îùº Îâ¥Ïä§", "channel_id": "UCXq7NNALDnqafn3KFvIyJKA"},
    {"Ï±ÑÎÑêÎ™Ö": "ÎßàÌîºÎîî ÎØ∏Íµ≠Ï£ºÏãù", "channel_id": "UCjp7GHSUKx9Joji3tIz1jCg"},
    {"Ï±ÑÎÑêÎ™Ö": "ÏóîÏßÄÎãàÏñ¥TV", "channel_id": "UCnvCYORRLMYMQNhrlBILdCg"},
    {"Ï±ÑÎÑêÎ™Ö": "Ïò¨ÎûúÎèÑ ÌÇ¥ ÎØ∏Íµ≠Ï£ºÏãù", "channel_id": "UCwSSqi-s0wcH6pJbH3YPZqQ"},
]

# SEC
SEC_BASE = "https://data.sec.gov"
SEC_UA = {"User-Agent": "TeslaDash/1.0 (your-email@example.com)"}  # Î≥∏Ïù∏ Ïù¥Î©îÏùºÎ°ú ÍµêÏ≤¥ Í∂åÏû•

# ÏïàÏ†ÑÌïú ÏãúÌÅ¨Î¶ø Ï†ëÍ∑º (secrets.toml ÏóÜÏùÑ ÎïåÎèÑ ÏïàÏ†Ñ)
def get_secret(key: str, default: str = "") -> str:
    try:
        return st.secrets.get(key, default)
    except Exception:
        return os.getenv(key, default) or default

X_BEARER = get_secret("X_BEARER_TOKEN", "")
YOUTUBE_API_KEY = get_secret("YOUTUBE_API_KEY", "")

# ---------------------------
# ÌéòÏù¥ÏßÄ ÏûêÎèô ÏÉàÎ°úÍ≥†Ïπ®(JS, Ï∂îÍ∞Ä ÏùòÏ°¥ÏÑ± ÁÑ°)
# ---------------------------
def auto_refresh_html(seconds: int, key: str):
    ms = int(seconds * 1000)
    st.components.v1.html(
        f"""
        <script>
        (function(){{
            const KEY="{key}";
            if(!window.__autoRefreshTimers) window.__autoRefreshTimers = {{}};
            if(!window.__autoRefreshTimers[KEY]) {{
                window.__autoRefreshTimers[KEY] = setTimeout(function(){{
                    const url = new URL(window.location.href);
                    url.searchParams.set(KEY, Date.now().toString());
                    window.location.href = url.toString();
                }}, {ms});
            }}
        }})();
        </script>
        """, height=0
    )

# ---------------------------
# Í∞ÄÍ≤©/Ï∞®Ìä∏ (Yahoo + Stooq fallback)
# ---------------------------
@st.cache_data(ttl=120)
def safe_yf_download(symbol: str, period: str, interval: str):
    """
    Í∞ïÏù∏Ìïú Í∞ÄÍ≤© ÏàòÏßë:
    1) yfinance.Ticker().history (Í∂åÏû•)
    2) yfinance.download (threads=False)
    3) interval/period ÏûêÎèô Î≥¥Ï†ï
    4) Stooq ÏùºÎ¥â ÏµúÏ¢Ö Î∞±ÏóÖ
    """
    import requests, pandas as pd, yfinance as yf
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})

    # üîß Í∞ÑÍ≤© Î≥¥Ï†ï: ÏùºÎ∂Ä ÌôòÍ≤ΩÏóêÏÑú '1h' ÎåÄÏã† '60m'Í∞Ä ÏïàÏ†ïÏ†Å
    interval_fixed = {"1h": "60m"}.get(interval, interval)

    # ÏöîÏ≤≠ ÌõÑÎ≥¥(Ïö∞ÏÑ†ÏàúÏúÑ)
    combos = [(period, interval_fixed)]
    if interval_fixed == "1m" and period not in {"1d","5d","7d"}:
        combos.append(("5d","1m"))
    if interval_fixed in INTRADAY and period not in {"1d","5d","7d","1mo","3mo","6mo","60d","90d"}:
        combos.append(("1mo","5m"))
    combos.append(("1y","1d"))  # ÏµúÌõÑÏùò ÏïºÌõÑ Îç∞ÏùºÎ¶¨

    last_err = None

    # 1) Ticker().history (Í∞ÄÏû• ÏïàÏ†ï)
    try:
        tkr = yf.Ticker(symbol, session=session)
        df = tkr.history(period=period, interval=interval_fixed, prepost=False, auto_adjust=False)
        if not df.empty:
            return df, (period, interval_fixed), "yfinance.history"
    except Exception as e:
        last_err = f"history: {e}"

    # 2) download Ïû¨ÏãúÎèÑ (threads=FalseÍ∞Ä Ïò§Î•ò Ï§ÑÏûÑ)
    for p, i in combos:
        try:
            df = yf.download(
                symbol, period=p, interval=i,
                auto_adjust=False, prepost=False,
                progress=False, threads=False, session=session
            )
            if isinstance(df.columns, pd.MultiIndex):
                df.columns = [c[0] for c in df.columns]
            if not df.empty:
                return df, (p, i), "yfinance.download"
        except Exception as e:
            last_err = f"download {p}/{i}: {e}"

    # 3) Stooq ÏùºÎ¥â Î∞±ÏóÖ
    try:
        import pandas as pd
        stooq = pd.read_csv("https://stooq.com/q/d/l/?s=tsla.us&i=d")
        stooq.rename(columns={"Date":"Datetime"}, inplace=True)
        stooq["Datetime"] = pd.to_datetime(stooq["Datetime"])
        stooq.set_index("Datetime", inplace=True)
        stooq = stooq.dropna()
        if not stooq.empty:
            return stooq, ("stooq-daily","1d"), "Yahoo empty ‚Üí Stooq daily fallback"
    except Exception as e:
        last_err = f"Stooq: {e}"

    # Î™®Îëê Ïã§Ìå®
    return pd.DataFrame(), None, last_err

def plot_candles(df: pd.DataFrame, title: str):
    if df.empty:
        st.error("Ï∞®Ìä∏ Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§.")
        return
    fig = go.Figure([go.Candlestick(
        x=df.index, open=df["Open"], high=df["High"], low=df["Low"], close=df["Close"], name="Price"
    )])
    if "Volume" in df.columns:
        fig.add_trace(go.Bar(x=df.index, y=df["Volume"], name="Volume", yaxis="y2", opacity=0.3))
        fig.update_layout(
            yaxis=dict(domain=[0.3,1.0], title="Price"),
            yaxis2=dict(domain=[0.0,0.25], title="Volume"),
            xaxis=dict(title="Time"),
            title=title, height=600, margin=dict(l=10, r=10, t=40, b=10)
        )
    else:
        fig.update_layout(title=title, height=550, margin=dict(l=10, r=10, t=40, b=10))
    st.plotly_chart(fig, use_container_width=True)

# ---------------------------
# Îâ¥Ïä§ RSS
# ---------------------------
@st.cache_data(ttl=300)
def fetch_rss(feed_url: str, limit: int = 12) -> List[Dict]:
    parsed = feedparser.parse(feed_url)
    items = []
    for e in parsed.entries[:limit]:
        items.append({
            "title": e.get("title"),
            "link": e.get("link"),
            "published": e.get("published", ""),
            "summary": re.sub("<.*?>","", e.get("summary","")) if e.get("summary") else "",
        })
    return items

# ---------------------------
# SEC 13F (Î¨¥Î£å)
# ---------------------------
def _strip_xml_ns(xml_text: str) -> str:
    return re.sub(r'\sxmlns(:\w+)?="[^"]+"','', xml_text)

def _to_int(x):
    try:
        return int(str(x).replace(",","").strip())
    except Exception:
        return None

@st.cache_data(ttl=3600)
def sec_recent_filings(cik: str) -> pd.DataFrame:
    cik10 = str(cik).zfill(10)
    url = f"{SEC_BASE}/submissions/CIK{cik10}.json"
    r = requests.get(url, headers=SEC_UA, timeout=20)
    r.raise_for_status()
    js = r.json()
    rec = js.get("filings", {}).get("recent", {})
    return pd.DataFrame(rec)

def _acc_nodash(acc: str) -> str:
    return acc.replace("-","")

@st.cache_data(ttl=3600)
def sec_list_13f_accessions(cik: str, limit=3) -> List[Dict]:
    df = sec_recent_filings(cik)
    if df.empty: return []
    mask = df["form"].isin(["13F-HR","13F-HR/A"])
    sdf = df[mask].head(limit)
    rows = []
    for _, r in sdf.iterrows():
        rows.append({
            "cik": cik,
            "accession": r["accessionNumber"],
            "reportDate": r.get("reportDate",""),
            "primaryDocument": r.get("primaryDocument",""),
        })
    return rows

@st.cache_data(ttl=3600)
def sec_find_infotable_url(cik: str, accession: str) -> Optional[str]:
    cik_nozero = str(int(cik)); acc = _acc_nodash(accession)
    idx = f"https://www.sec.gov/Archives/edgar/data/{cik_nozero}/{acc}/index.json"
    r = requests.get(idx, headers=SEC_UA, timeout=20)
    if not r.ok: return None
    files = r.json().get("directory",{}).get("item",[])
    for f in files:
        name = f.get("name","").lower()
        if name.endswith(".xml") and ("infotable" in name or "informationtable" in name or "form13f" in name):
            return f"https://www.sec.gov/Archives/edgar/data/{cik_nozero}/{acc}/{f['name']}"
    for f in files:
        name = f.get("name","").lower()
        if name.endswith(".txt"):
            return f"https://www.sec.gov/Archives/edgar/data/{cik_nozero}/{acc}/{f['name']}"
    return None

def _parse_infotable_xml(xml_text: str) -> pd.DataFrame:
    m = re.search(r"<informationTable[\s\S]*</informationTable>", xml_text, re.IGNORECASE)
    if m: xml_text = m.group(0)
    xml_text = _strip_xml_ns(xml_text)
    root = ET.fromstring(xml_text.encode("utf-8"))
    rows = []
    for it in root.iterfind(".//infoTable"):
        issuer = (it.findtext("nameOfIssuer","") or "").strip()
        cusip = (it.findtext("cusip","") or "").strip()
        amt = it.find(".//shrsOrPrnAmt/sshPrnamt")
        shares = _to_int(amt.text) if (amt is not None and amt.text) else None
        val = _to_int(it.findtext("value"))
        value_usd = val*1000 if val is not None else None
        rows.append({"issuer":issuer,"cusip":cusip,"shares":shares,"value_usd":value_usd})
    return pd.DataFrame(rows)

@st.cache_data(ttl=3600)
def sec_tsla_position_from_13f(cik: str, accession: str) -> Optional[Dict]:
    url = sec_find_infotable_url(cik, accession)
    if not url: return None
    r = requests.get(url, headers=SEC_UA, timeout=30)
    if not r.ok or not r.text: return None
    try:
        df = _parse_infotable_xml(r.text)
    except Exception:
        return None
    if df.empty:
        return {"shares":0, "value_usd":0}
    m = df[(df["cusip"].str.startswith(CUSIP_TSLA_PREFIX, na=False)) |
           (df["issuer"].str.contains("TESLA", case=False, na=False))]
    if m.empty:
        return {"shares":0, "value_usd":0}
    return {
        "shares": int(m["shares"].fillna(0).sum()),
        "value_usd": int(m["value_usd"].fillna(0).sum())
    }

def build_13f_table(managers: Dict[str, str]) -> pd.DataFrame:
    out = []
    for name, cik in managers.items():
        try:
            accs = sec_list_13f_accessions(cik, limit=2)
            if not accs: continue
            latest = accs[0]; prev = accs[1] if len(accs)>1 else None
            latest_pos = sec_tsla_position_from_13f(cik, latest["accession"])
            time.sleep(0.4)
            prev_pos = sec_tsla_position_from_13f(cik, prev["accession"]) if prev else None
            shares = latest_pos["shares"] if latest_pos else None
            value_usd = latest_pos["value_usd"] if latest_pos else None
            delta = None
            if latest_pos and prev_pos:
                delta = (latest_pos["shares"] or 0) - (prev_pos["shares"] or 0)
            out.append({
                "Í∏∞Í¥Ä/ÌéÄÎìú": name,
                "CIK": cik,
                "Î≥¥Í≥†Ïùº(ÏµúÍ∑º)": latest.get("reportDate",""),
                "Î≥¥Ïú†Ï£ºÏàò(ÏµúÍ∑º)": shares,
                "ÌèâÍ∞ÄÏï°(USD, ÏµúÍ∑º)": value_usd,
                "Î≥¥Ïú†Ï£ºÏàò Ï¶ùÍ∞ê(qoq)": delta
            })
        except Exception:
            continue
    df = pd.DataFrame(out)
    if not df.empty:
        df = df.sort_values(by=["Î≥¥Ïú†Ï£ºÏàò(ÏµúÍ∑º)"], ascending=False)
    return df

# ---------------------------
# X API (ÏÑ†ÌÉù, Ïù¥ÎØ∏ ÏÇ¨Ïö© Ï§ëÏù∏ Íµ¨Ï°∞ Ïú†ÏßÄ)
# ---------------------------
def _x_headers():
    if not X_BEARER: return None
    return {"Authorization": f"Bearer {X_BEARER}"}

def _x_api_get(url, params=None, timeout=15):
    h = _x_headers()
    if not h: return None
    for base in ("https://api.x.com", "https://api.twitter.com"):
        try:
            r = requests.get(base+url, headers=h, params=params, timeout=timeout)
            if r.ok: return r.json()
        except Exception:
            pass
    return None

@st.cache_data(ttl=24*3600)
def x_get_user_id(username: str) -> Optional[str]:
    if username in X_USER_IDS: return X_USER_IDS[username]
    js = _x_api_get(f"/2/users/by/username/{username}")
    if not js: return None
    return js.get("data",{}).get("id")

@st.cache_data(ttl=0)
def x_fetch_latest_tweets(user_id: str, since_id: Optional[str]=None, max_results: int=5):
    params = {"max_results":str(max_results), "exclude":"retweets,replies",
              "tweet.fields":"created_at,public_metrics,entities"}
    if since_id: params["since_id"] = since_id
    js = _x_api_get(f"/2/users/{user_id}/tweets", params=params)
    if not js: return [], since_id
    data = js.get("data", [])
    data.sort(key=lambda t: t.get("id"), reverse=False)
    new_since = data[-1]["id"] if data else since_id
    return data, new_since

def _format_tweet_text(t):
    txt = t.get("text","")
    ents = (t.get("entities") or {}).get("urls", []) or []
    for url in ents:
        u = url.get("url"); ex = url.get("expanded_url") or u
        if u: txt = txt.replace(u, ex)
    return txt

# ---------------------------
# YouTube (RSS + Data API v3)
# ---------------------------
@st.cache_data(ttl=300)
def yt_rss_latest(channel_id: str, limit: int = 6):
    """API ÌÇ§ ÏóÜÏù¥ ÏµúÏã† ÏòÅÏÉÅ Î¶¨Ïä§Ìä∏"""
    feed = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    parsed = feedparser.parse(feed)
    items = []
    for e in parsed.entries[:limit]:
        vid = e.get("yt_videoid")
        if not vid:
            # ÎßÅÌÅ¨ÏóêÏÑú v ÌååÎùºÎØ∏ÌÑ∞ Ï∂îÏ∂ú
            try:
                q = parse_qs(urlparse(e.get("link","")).query)
                vid = q.get("v",[None])[0]
            except Exception:
                vid = None
        items.append({
            "video_id": vid,
            "title": e.get("title",""),
            "link": e.get("link",""),
            "published": e.get("published",""),
        })
    return items

@st.cache_data(ttl=60)
def yt_api_live_videos(channel_id: str, max_results: int = 3):
    """API ÌÇ§Í∞Ä ÏûàÏùÑ Îïå Ìï¥Îãπ Ï±ÑÎÑêÏùò ÎùºÏù¥Î∏å Ï§ëÏù∏ ÏòÅÏÉÅ Í≤ÄÏÉâ"""
    if not YOUTUBE_API_KEY:
        return []
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "part": "snippet",
        "channelId": channel_id,
        "eventType": "live",
        "type": "video",
        "order": "date",
        "maxResults": str(max_results),
        "key": YOUTUBE_API_KEY,
    }
    r = requests.get(url, params=params, timeout=15)
    if not r.ok:
        return []
    data = r.json().get("items", [])
    out = []
    for it in data:
        vid = it.get("id",{}).get("videoId")
        sn = it.get("snippet",{})
        out.append({
            "video_id": vid,
            "title": sn.get("title","(live)"),
            "published": sn.get("publishedAt",""),
            "link": f"https://www.youtube.com/watch?v={vid}" if vid else "",
        })
    return out

@st.cache_data(ttl=300)
def yt_api_latest_videos(channel_id: str, max_results: int = 6):
    """API ÌÇ§Í∞Ä ÏûàÏúºÎ©¥ Í≤ÄÏÉâ APIÎ°ú ÏµúÏã† ÏòÅÏÉÅ(ÏóÖÎ°úÎìú) Ï°∞Ìöå; ÏóÜÏúºÎ©¥ RSSÎ•º Ïì∞Îäî Í≤å ÎÇ´Îã§."""
    if not YOUTUBE_API_KEY:
        return yt_rss_latest(channel_id, max_results)
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "part": "snippet",
        "channelId": channel_id,
        "type": "video",
        "order": "date",
        "maxResults": str(max_results),
        "key": YOUTUBE_API_KEY,
    }
    r = requests.get(url, params=params, timeout=15)
    if not r.ok:
        return yt_rss_latest(channel_id, max_results)
    data = r.json().get("items", [])
    out = []
    for it in data:
        vid = it.get("id",{}).get("videoId")
        sn = it.get("snippet",{})
        out.append({
            "video_id": vid,
            "title": sn.get("title",""),
            "published": sn.get("publishedAt",""),
            "link": f"https://www.youtube.com/watch?v={vid}" if vid else "",
        })
    return out

def yt_embed(video_id: str, height: int = 315):
    if not video_id:
        return
    st.components.v1.html(
        f"""
        <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;">
          <iframe src="https://www.youtube.com/embed/{video_id}"
                  title="YouTube video" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
        </div>
        """,
        height=height+60
    )

# ---------------------------
# UI
# ---------------------------
st.sidebar.title("Tesla One-Stop (Free)")
page = st.sidebar.radio(
    "Î©îÎâ¥",
    ["üìà Ï∞®Ìä∏", "üì∞ Îâ¥Ïä§/ÏΩîÎ©òÌä∏", "üì∫ Ïú†ÌäúÎ∏å", "üè¶ ÏßÄÎ∂Ñ Î≥ÄÎèô(13F, Î¨¥Î£å)", "‚öôÔ∏è ÏòµÏÖò/ÏÑ§Ï†ï"]
)

# ---- Ï∞®Ìä∏ ----
if page == "üìà Ï∞®Ìä∏":
    st.title("üìà TSLA Ï∞®Ìä∏ (ÏïàÏ†ïÌôî Î≤ÑÏ†Ñ)")
    c1,c2,c3 = st.columns(3)
    with c1:
        period = st.selectbox("Í∏∞Í∞Ñ", ["1d","5d","7d","1mo","3mo","6mo","1y"], index=0)
    with c2:
        interval = st.selectbox("Î¥â Í∞ÑÍ≤©", ["1m","5m","15m","1h","1d"], index=0)
    with c3:
        if st.button("Ï∫êÏãú Ï¥àÍ∏∞Ìôî"):
            st.cache_data.clear()
            st.success("Ï∫êÏãú ÏÇ≠Ï†ú ÏôÑÎ£å. Îã§Ïãú Î∂àÎü¨Ïò§Îäî Ï§ë‚Ä¶")
    df, used, note = safe_yf_download("TSLA", period, interval)
    if note: st.caption(f"ÏÜåÏä§: {note}")
    if df.empty:
        st.error("ÏãúÏÑ∏Î•º Í∞ÄÏ†∏Ïò§ÏßÄ Î™ªÌñàÏñ¥Ïöî.")
        with st.expander("ÎÑ§Ìä∏ÏõåÌÅ¨ ÏßÑÎã®"):
            import requests
            tests = [
                "https://query2.finance.yahoo.com/v1/finance/trending/US?count=1",
                "https://query2.finance.yahoo.com/v8/finance/chart/TSLA?range=1d&interval=1m",
                "https://stooq.com/q/d/l/?s=tsla.us&i=d",
            ]
            for u in tests:
                try:
                    r = requests.get(u, headers={"User-Agent":"Mozilla/5.0"}, timeout=6)
                    st.write(u, "‚Üí", r.status_code, f"{len(r.content)} bytes")
                except Exception as e:
                    st.write(u, "‚Üí", str(e))
    else:
        plot_candles(df, f"TSLA {used[0]}/{used[1]}")
# ---- Îâ¥Ïä§/X ----
elif page == "üì∞ Îâ¥Ïä§/ÏΩîÎ©òÌä∏":
    st.title("üì∞ Îâ¥Ïä§ & ÏΩîÎ©òÌä∏")
    t1,t2 = st.tabs(["Îâ¥Ïä§ RSS","Ïú†Î™ÖÏù∏ ÏΩîÎ©òÌä∏ (X ÏûÑÎ≤†Îìú Î¶¨ÌîÑÎ†àÏãú)"])
    with t1:
        cols = st.columns(2)
        keys = list(RSS_SOURCES.keys())
        left, right = keys[:(len(keys)+1)//2], keys[(len(keys)+1)//2:]
        for col, group in zip(cols, [left, right]):
            with col:
                for k in group:
                    st.subheader(k)
                    for it in fetch_rss(RSS_SOURCES[k], limit=7):
                        st.markdown(f"- **[{it['title']}]({it['link']})**")
                        if it["published"]: st.caption(it["published"])
                    st.markdown("---")
    with t2:
        acct_label = st.selectbox("Í≥ÑÏ†ï ÏÑ†ÌÉù", list(X_USERNAMES.keys()))
        handle = X_USERNAMES[acct_label]
        refresh_sec = st.slider("ÏÉàÎ°úÍ≥†Ïπ®(Ï¥à)", 15, 180, 60, step=15)
        auto_refresh_html(refresh_sec, key=f"x_refresh_{handle}")
        embed_html = f"""
        <a class="twitter-timeline" href="https://twitter.com/{handle}?ref_src=twsrc%5Etfw">
          Tweets by @{handle}
        </a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        """
        st.components.v1.html(embed_html, height=800, scrolling=True)

# ---- Ïú†ÌäúÎ∏å ----
elif page == "üì∫ Ïú†ÌäúÎ∏å":
    st.title("üì∫ ÌÖåÏä¨Îùº Ïú†ÌäúÎ≤Ñ ‚Äî ÎùºÏù¥Î∏å/ÏµúÏã† ÏòÅÏÉÅ")

    # 1) Ï±ÑÎÑê Î™©Î°ù Ìé∏Ïßë
    st.subheader("Ï±ÑÎÑê Î™©Î°ù Ìé∏Ïßë")
    df_channels = st.session_state.get("yt_channels_df") or pd.DataFrame(DEFAULT_YT_CHANNELS)
    df_channels = st.data_editor(df_channels, num_rows="dynamic", key="yt_channels_editor")
    st.session_state["yt_channels_df"] = df_channels

    st.markdown("---")

    # 2) ÎùºÏù¥Î∏å Ï≤¥ÌÅ¨(ÏÑ†ÌÉù)
    st.subheader("ÏßÄÍ∏à ÎùºÏù¥Î∏å Ï§ë üî¥")
    refresh_live = st.checkbox("ÏûêÎèô ÏÉàÎ°úÍ≥†Ïπ®(Ï¥à) ÏÑ§Ï†ï", value=True)
    live_interval = st.slider("ÎùºÏù¥Î∏å Ï≤¥ÌÅ¨ Ï£ºÍ∏∞(Ï¥à)", 30, 180, 60, step=15, disabled=not refresh_live)
    if refresh_live:
        auto_refresh_html(live_interval, key="yt_live_refresh")

    if df_channels.empty:
        st.info("Ï±ÑÎÑêÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî. Ïòà: channel_id = UC_x5XG1OV2P6uZZ5FSM9TtQ")
    else:
        if not YOUTUBE_API_KEY:
            st.info("YOUTUBE_API_KEYÍ∞Ä ÏóÜÏñ¥ÏÑú ÎùºÏù¥Î∏å ÏÉÅÌÉúÎäî API ÏóÜÏù¥ ÌôïÏù∏Ìï©ÎãàÎã§. Í∞Å Ï±ÑÎÑêÏùò `/live` ÎßÅÌÅ¨Î•º ÎàåÎü¨ ÌôïÏù∏ÌïòÏÑ∏Ïöî.")
        live_cols = st.columns(3)
        idx = 0
        for _, row in df_channels.iterrows():
            name = str(row.get("Ï±ÑÎÑêÎ™Ö","")).strip()
            cid  = str(row.get("channel_id","")).strip()
            if not cid: continue
            # APIÍ∞Ä ÏûàÏúºÎ©¥ ÎùºÏù¥Î∏å Í≤ÄÏÉâ
            lives = yt_api_live_videos(cid, max_results=2) if YOUTUBE_API_KEY else []
            with live_cols[idx % 3]:
                if lives:
                    for lv in lives:
                        st.markdown(f"**{name}** ‚Äî üî¥ LIVE: [{lv['title']}]({lv['link']})")
                        yt_embed(lv["video_id"])
                else:
                    st.markdown(f"**{name}** ‚Äî ÌòÑÏû¨ ÎùºÏù¥Î∏å Í∞êÏßÄ ÏóÜÏùå")
                    st.caption(f"[Ï±ÑÎÑê ÎùºÏù¥Î∏å ÌéòÏù¥ÏßÄ Î∞îÎ°úÍ∞ÄÍ∏∞](https://www.youtube.com/channel/{cid}/live)")
            idx += 1

    st.markdown("---")

    # 3) ÏµúÏã† ÏóÖÎ°úÎìú
    st.subheader("ÏµúÏã† ÏóÖÎ°úÎìú")
    per_channel = st.slider("Ï±ÑÎÑêÎ≥Ñ ÌëúÏãú Í∞úÏàò", 1, 8, 3)
    refresh_latest = st.checkbox("ÏµúÏã† ÏòÅÏÉÅ ÏûêÎèô ÏÉàÎ°úÍ≥†Ïπ®", value=False)
    if refresh_latest:
        auto_refresh_html(90, key="yt_latest_refresh")

    if not df_channels.empty:
        for _, row in df_channels.iterrows():
            name = str(row.get("Ï±ÑÎÑêÎ™Ö","")).strip()
            cid  = str(row.get("channel_id","")).strip()
            if not cid: continue
            st.markdown(f"### {name}")
            vids = yt_api_latest_videos(cid, max_results=per_channel)
            if not vids:
                st.write("ÏòÅÏÉÅ Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§.")
                continue
            cards = st.columns(len(vids))
            for col, v in zip(cards, vids):
                with col:
                    st.markdown(f"**[{v['title']}]({v['link']})**")
                    if v["video_id"]:
                        yt_embed(v["video_id"], height=200)
                    if v.get("published"):
                        st.caption(v["published"])
            st.markdown("---")

    st.caption("ÌåÅ: Ï±ÑÎÑê IDÎäî Ï±ÑÎÑê ÌéòÏù¥ÏßÄ URLÏóêÏÑú `channel/` Îí§ 24Ïûê IDÏûÖÎãàÎã§. Ïª§Ïä§ÌÖÄ Ìï∏Îì§Ïù¥Î©¥ 'Ï±ÑÎÑê Ï†ïÎ≥¥ > Í≥µÏú† > Ï±ÑÎÑê ÎßÅÌÅ¨'Î°ú Ïã§Ï†ú ID ÌôïÏù∏.")

# ---- 13F (Î¨¥Î£å)
elif page == "üè¶ ÏßÄÎ∂Ñ Î≥ÄÎèô(13F, Î¨¥Î£å)":
    st.title("üè¶ Í∏∞Í¥Ä Î≥¥Ïú†/Î≥ÄÎèô ‚Äî SEC 13F (ÏôÑÏ†Ñ Î¨¥Î£å)")
    with st.expander("ÎåÄÏÉÅ Í∏∞Í¥Ä(Ìé∏Ïßë Í∞ÄÎä•)"):
        edit_df = pd.DataFrame([{"Í∏∞Í¥Ä/ÌéÄÎìú": k, "CIK": v} for k,v in DEFAULT_CIKS.items()])
        edited = st.data_editor(edit_df, num_rows="dynamic", key="mgr_edit")
        managers = {row["Í∏∞Í¥Ä/ÌéÄÎìú"]: str(row["CIK"]) for _, row in edited.iterrows()
                    if row.get("Í∏∞Í¥Ä/ÌéÄÎìú") and row.get("CIK")}
    if st.button("13F ÏÉàÎ°ú Í≥†Ïπ®", type="primary"):
        st.cache_data.clear()
    with st.spinner("SECÏóêÏÑú 13F Î∂àÎü¨Ïò§Îäî Ï§ë..."):
        df = build_13f_table(managers)
    if df.empty:
        st.warning("Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§.")
    else:
        fmt = df.copy()
        if "Î≥¥Ïú†Ï£ºÏàò(ÏµúÍ∑º)" in fmt:
            fmt["Î≥¥Ïú†Ï£ºÏàò(ÏµúÍ∑º)"] = fmt["Î≥¥Ïú†Ï£ºÏàò(ÏµúÍ∑º)"].map(lambda x: f"{x:,}" if pd.notna(x) else "")
        if "ÌèâÍ∞ÄÏï°(USD, ÏµúÍ∑º)" in fmt:
            fmt["ÌèâÍ∞ÄÏï°(USD, ÏµúÍ∑º)"] = fmt["ÌèâÍ∞ÄÏï°(USD, ÏµúÍ∑º)"].map(lambda x: f"${x:,}" if pd.notna(x) else "")
        if "Î≥¥Ïú†Ï£ºÏàò Ï¶ùÍ∞ê(qoq)" in fmt:
            fmt["Î≥¥Ïú†Ï£ºÏàò Ï¶ùÍ∞ê(qoq)"] = fmt["Î≥¥Ïú†Ï£ºÏàò Ï¶ùÍ∞ê(qoq)"].map(lambda x: f"{x:+,}" if pd.notna(x) else "")
        st.dataframe(fmt, use_container_width=True)
    st.info("Ï∞∏Í≥†: 13FÎäî Î∂ÑÍ∏∞ Îã®ÏúÑ Í≥µÍ∞úÏù¥Î©∞, ÏùºÏ§ë Î≥ÄÎèôÏùÄ Ï†úÍ≥µÎêòÏßÄ ÏïäÏäµÎãàÎã§.")

# ---- ÏÑ§Ï†ï
elif page == "‚öôÔ∏è ÏòµÏÖò/ÏÑ§Ï†ï":
    st.title("‚öôÔ∏è ÏòµÏÖò/ÏÑ§Ï†ï")
    st.markdown("""
    - Ïú†ÌäúÎ∏å: **API ÌÇ§ ÏóÜÏù¥ÎèÑ** ÏµúÏã† ÏòÅÏÉÅÏùÄ RSSÎ°ú ÌëúÏãúÎê©ÎãàÎã§. ÎùºÏù¥Î∏å Í∞êÏßÄÎäî **YOUTUBE_API_KEY**Í∞Ä ÏûàÏùÑ Îïå ÏûêÎèôÌôîÎê©ÎãàÎã§.
    - SEC: User-AgentÏóê **Ïó∞ÎùΩ Í∞ÄÎä•Ìïú Ïù¥Î©îÏùº** ÌëúÍ∏∞Î•º Í∂åÏû•Ìï©ÎãàÎã§.
    - X API Ìè¥ÎßÅÏùÑ Ïì∞Î©¥ X_BEARER_TOKENÏù¥ ÌïÑÏöîÌï©ÎãàÎã§(ÏóÜÏñ¥ÎèÑ ÏûÑÎ≤†ÎìúÎäî ÎèôÏûë).
    """)
