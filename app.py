# app.py â€” Tesla One-Stop (FREE) + X ì„ë² ë“œ/í´ë§
import time
import re
from datetime import datetime
from typing import List, Dict, Optional
import os

import pandas as pd
import requests
import yfinance as yf
import feedparser
import plotly.graph_objects as go
import streamlit as st
from streamlit_autorefresh import st_autorefresh
from xml.etree import ElementTree as ET

# ---------------------------
# Streamlit Config
# ---------------------------
st.set_page_config(page_title="Tesla One-Stop (Free)", page_icon="ğŸš—", layout="wide")

# ---------------------------
# Constants & Settings
# ---------------------------
TSLA = "TSLA"
CUSIP_TSLA_PREFIX = "88160R"  # TSLA CUSIP prefix
INTRADAY = {"1m", "2m", "5m", "15m", "30m", "60m", "90m"}

# ê¸°ë³¸ ì¶”ì  ê¸°ê´€(CIK)
DEFAULT_CIKS = {
    "BlackRock Inc.": "0001364742",
    "The Vanguard Group, Inc.": "0000102909",
    "FMR LLC (Fidelity)": "0000315066",
    "State Street Corp.": "0000093751",
    "ARK Investment Management LLC": "0001697747",
    "T. Rowe Price Associates, Inc.": "0000080255",
}

RSS_SOURCES = {
    "Tesla IR (Press)": "https://ir.tesla.com/press-releases/rss",
    "Electrek â€“ Tesla": "https://electrek.co/guides/tesla/feed/",
    "Reuters â€“ Tesla": "https://feeds.reuters.com/reuters/businessNews?query=tesla",
    "CNBC â€“ Tesla": "https://www.cnbc.com/id/15839135/device/rss/rss.html?query=tesla",
    "Google News â€“ Tesla": "https://news.google.com/rss/search?q=Tesla%20OR%20TSLA&hl=en-US&gl=US&ceid=US:en",
}

# X(íŠ¸ìœ„í„°) í•¸ë“¤/ID
X_USERNAMES = {
    "Elon Musk": "elonmusk",
    "Donald Trump": "realDonaldTrump",
    "Tesla": "Tesla",
}
# API í˜¸ì¶œ ìˆ˜ ì ˆê°ìš©: ì˜ ì•Œë ¤ì§„ ê³ ì • user_id í•˜ë“œì½”ë”©(ì—†ìœ¼ë©´ ìë™ ì¡°íšŒ)
X_USER_IDS = {
    "elonmusk": "44196397",
    "realDonaldTrump": "25073877",
    "Tesla": "13298072",
}

# ---- SEC base (ë¬´ë£Œ 13F) ----
SEC_BASE = "https://data.sec.gov"
# âš ï¸ ê¼­ ë³¸ì¸ ì´ë©”ì¼ë¡œ êµì²´í•˜ì„¸ìš”(SEC ê¶Œì¥)
SEC_UA = {"User-Agent": "TeslaDash/1.0 (your-email@example.com)"}

# X API (ì„ íƒ)
def get_secret(key: str, default: str = "") -> str:
    try:
        return st.secrets.get(key, default)  # secrets.tomlì´ ì—†ìœ¼ë©´ ì˜ˆì™¸ê°€ ë‚  ìˆ˜ ìˆìŒ
    except Exception:
        return os.getenv(key, default) or default

X_BEARER = get_secret("X_BEARER_TOKEN", "")  # â¬…ï¸ ê¸°ì¡´ st.secrets.get(...) ëŒ€ì‹  ì´ê±¸ ì‚¬ìš©

# ---------------------------
# Utils â€” Price (Yahoo + Stooq fallback)
# ---------------------------
@st.cache_data(ttl=120)
def safe_yf_download(symbol: str, period: str, interval: str):
    """
    ì•¼í›„ ê·œì¹™ì— ë§ê²Œ period/interval ìë™ ë³´ì • + ì„¸ì…˜ UA ì£¼ì… + ì¬ì‹œë„ + Stooq ì¼ë´‰ ë°±ì—…
    ë°˜í™˜: (df, (used_period, used_interval), note|None)
    """
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})
    combos = [(period, interval)]

    # ë³´ì • í›„ë³´ ì¶”ê°€
    if interval == "1m" and period not in {"1d", "5d", "7d"}:
        combos.append(("5d", "1m"))
    if interval in INTRADAY and period not in {"1d", "5d", "7d", "1mo", "3mo", "6mo", "60d", "90d"}:
        combos.append(("1mo", "5m"))
    combos.append(("1y", "1d"))  # ìµœí›„ì˜ ì•¼í›„ ë°ì¼ë¦¬ ìš”ì²­

    last_err = None
    for p, i in combos:
        try:
            df = yf.download(
                symbol, period=p, interval=i,
                auto_adjust=False, prepost=False, progress=False, session=session
            )
            if isinstance(df.columns, pd.MultiIndex):
                df.columns = [c[0] for c in df.columns]
            if not df.empty:
                return df, (p, i), None
        except Exception as e:
            last_err = str(e)

    # Yahoo ì‹¤íŒ¨ ì‹œ Stooq ì¼ë´‰ ë°±ì—…
    try:
        stooq = pd.read_csv("https://stooq.com/q/d/l/?s=tsla.us&i=d")
        stooq.rename(columns={
            "Date": "Datetime",
            "Open": "Open", "High": "High", "Low": "Low", "Close": "Close", "Volume": "Volume"
        }, inplace=True)
        stooq["Datetime"] = pd.to_datetime(stooq["Datetime"])
        stooq.set_index("Datetime", inplace=True)
        stooq = stooq.dropna()
        if not stooq.empty:
            return stooq, ("stooq-daily", "1d"), "Yahoo empty â†’ Stooq daily fallback"
    except Exception as e:
        last_err = f"Yahoo+Stooq failed: {e}"

    return pd.DataFrame(), None, last_err

def plot_candles(df: pd.DataFrame, title: str):
    if df.empty:
        st.error("ì°¨íŠ¸ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return
    fig = go.Figure([go.Candlestick(
        x=df.index, open=df["Open"], high=df["High"], low=df["Low"], close=df["Close"], name="Price"
    )])
    if "Volume" in df.columns:
        fig.add_trace(go.Bar(x=df.index, y=df["Volume"], name="Volume", yaxis="y2", opacity=0.3))
        fig.update_layout(
            yaxis=dict(domain=[0.3, 1.0], title="Price"),
            yaxis2=dict(domain=[0.0, 0.25], title="Volume"),
            xaxis=dict(title="Time"),
            title=title, height=600, margin=dict(l=10, r=10, t=40, b=10)
        )
    else:
        fig.update_layout(title=title, height=550, margin=dict(l=10, r=10, t=40, b=10))
    st.plotly_chart(fig, use_container_width=True)

# ---------------------------
# Utils â€” RSS
# ---------------------------
@st.cache_data(ttl=300)
def fetch_rss(feed_url: str, limit: int = 12) -> List[Dict]:
    parsed = feedparser.parse(feed_url)
    items = []
    for e in parsed.entries[:limit]:
        items.append({
            "title": e.get("title"),
            "link": e.get("link"),
            "published": e.get("published", ""),
            "summary": re.sub("<.*?>", "", e.get("summary", "")) if e.get("summary") else "",
        })
    return items

# ---------------------------
# Utils â€” SEC 13F (ë¬´ë£Œ)
# ---------------------------
def _strip_xml_ns(xml_text: str) -> str:
    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±° (íŒŒì‹± í˜¸í™˜ì„±â†‘)
    xml_text = re.sub(r'\sxmlns(:\w+)?="[^"]+"', '', xml_text)
    return xml_text

def _to_int(x):
    try:
        return int(str(x).replace(",", "").strip())
    except Exception:
        return None

@st.cache_data(ttl=3600)
def sec_recent_filings(cik: str) -> pd.DataFrame:
    cik10 = str(cik).zfill(10)
    url = f"{SEC_BASE}/submissions/CIK{cik10}.json"
    r = requests.get(url, headers=SEC_UA, timeout=20)
    r.raise_for_status()
    js = r.json()
    rec = js.get("filings", {}).get("recent", {})
    df = pd.DataFrame(rec)
    return df

def _acc_nodash(acc: str) -> str:
    return acc.replace("-", "")

@st.cache_data(ttl=3600)
def sec_list_13f_accessions(cik: str, limit=3) -> List[Dict]:
    df = sec_recent_filings(cik)
    if df.empty:
        return []
    mask = df["form"].isin(["13F-HR", "13F-HR/A"])
    sdf = df[mask].head(limit)
    rows = []
    for _, r in sdf.iterrows():
        rows.append({
            "cik": cik,
            "accession": r["accessionNumber"],
            "reportDate": r.get("reportDate", ""),
            "primaryDocument": r.get("primaryDocument", ""),
        })
    return rows

@st.cache_data(ttl=3600)
def sec_find_infotable_url(cik: str, accession: str) -> Optional[str]:
    cik_nozero = str(int(cik))
    acc_nodash = _acc_nodash(accession)
    idx = f"https://www.sec.gov/Archives/edgar/data/{cik_nozero}/{acc_nodash}/index.json"
    r = requests.get(idx, headers=SEC_UA, timeout=20)
    if not r.ok:
        return None
    files = r.json().get("directory", {}).get("item", [])
    # XML ìš°ì„ 
    for f in files:
        name = f.get("name", "").lower()
        if name.endswith(".xml") and ("infotable" in name or "informationtable" in name or "form13f" in name):
            return f"https://www.sec.gov/Archives/edgar/data/{cik_nozero}/{acc_nodash}/{f['name']}"
    # TXT ë°±ì—… (XMLì´ txt ë‚´ë¶€ì— í¬í•¨ëœ ì¼€ì´ìŠ¤)
    for f in files:
        name = f.get("name", "").lower()
        if name.endswith(".txt"):
            return f"https://www.sec.gov/Archives/edgar/data/{cik_nozero}/{acc_nodash}/{f['name']}"
    return None

def _parse_infotable_xml(xml_text: str) -> pd.DataFrame:
    # TXT ì•ˆì˜ <informationTable>ë§Œ ì¶”ì¶œ
    m = re.search(r"<informationTable[\s\S]*</informationTable>", xml_text, re.IGNORECASE)
    if m:
        xml_text = m.group(0)
    xml_text = _strip_xml_ns(xml_text)
    root = ET.fromstring(xml_text.encode("utf-8"))
    rows = []
    for it in root.iterfind(".//infoTable"):
        issuer = (it.findtext("nameOfIssuer", default="") or "").strip()
        cusip = (it.findtext("cusip", default="") or "").strip()
        # shares
        amt = it.find(".//shrsOrPrnAmt/sshPrnamt")
        shares = _to_int(amt.text) if amt is not None and amt.text else None
        # value (thousands USD)
        val = _to_int(it.findtext("value"))
        value_usd = val * 1000 if val is not None else None
        rows.append({"issuer": issuer, "cusip": cusip, "shares": shares, "value_usd": value_usd})
    return pd.DataFrame(rows)

@st.cache_data(ttl=3600)
def sec_tsla_position_from_13f(cik: str, accession: str) -> Optional[Dict]:
    url = sec_find_infotable_url(cik, accession)
    if not url:
        return None
    r = requests.get(url, headers=SEC_UA, timeout=30)
    if not r.ok or not r.text:
        return None
    try:
        df = _parse_infotable_xml(r.text)
    except Exception:
        return None
    if df.empty:
        return {"shares": 0, "value_usd": 0}
    m = df[(df["cusip"].str.startswith(CUSIP_TSLA_PREFIX, na=False)) |
           (df["issuer"].str.contains("TESLA", case=False, na=False))]
    if m.empty:
        return {"shares": 0, "value_usd": 0}
    return {
        "shares": int(m["shares"].fillna(0).sum()),
        "value_usd": int(m["value_usd"].fillna(0).sum())
    }

def build_13f_table(managers: Dict[str, str]) -> pd.DataFrame:
    out = []
    for name, cik in managers.items():
        try:
            accs = sec_list_13f_accessions(cik, limit=2)
            if not accs:
                continue
            latest = accs[0]
            prev = accs[1] if len(accs) > 1 else None
            latest_pos = sec_tsla_position_from_13f(cik, latest["accession"])
            time.sleep(0.4)  # SEC ì˜ˆì˜ìƒ
            prev_pos = sec_tsla_position_from_13f(cik, prev["accession"]) if prev else None

            shares = latest_pos["shares"] if latest_pos else None
            value_usd = latest_pos["value_usd"] if latest_pos else None
            delta = None
            if latest_pos and prev_pos:
                delta = (latest_pos["shares"] or 0) - (prev_pos["shares"] or 0)

            out.append({
                "ê¸°ê´€/í€ë“œ": name,
                "CIK": cik,
                "ë³´ê³ ì¼(ìµœê·¼)": latest.get("reportDate", ""),
                "ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)": shares,
                "í‰ê°€ì•¡(USD, ìµœê·¼)": value_usd,
                "ë³´ìœ ì£¼ìˆ˜ ì¦ê°(qoq)": delta
            })
        except Exception:
            continue
    df = pd.DataFrame(out)
    if not df.empty:
        df = df.sort_values(by=["ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)"], ascending=False)
    return df

# ---------------------------
# Utils â€” X API (ì„ íƒ, í´ë§)
# ---------------------------
def _x_headers():
    if not X_BEARER:
        return None
    return {"Authorization": f"Bearer {X_BEARER}"}

def _x_api_get(url, params=None, timeout=15):
    """
    api.x.com -> ì‹¤íŒ¨ ì‹œ api.twitter.comë„ ì‹œë„(ì¼ë¶€ í™˜ê²½ í˜¸í™˜)
    """
    h = _x_headers()
    if not h:
        return None
    for base in ("https://api.x.com", "https://api.twitter.com"):
        try:
            r = requests.get(base + url, headers=h, params=params, timeout=timeout)
            if r.ok:
                return r.json()
        except Exception:
            pass
    return None

@st.cache_data(ttl=24*3600)  # í•˜ë£¨ ìºì‹œ
def x_get_user_id(username: str) -> Optional[str]:
    # í•˜ë“œì½”ë”© ìš°ì„  ì‚¬ìš©
    if username in X_USER_IDS:
        return X_USER_IDS[username]
    js = _x_api_get(f"/2/users/by/username/{username}")
    if not js:
        return None
    return js.get("data", {}).get("id")

@st.cache_data(ttl=0)  # í´ë§ ì£¼ê¸°ë§ˆë‹¤ ìƒˆ í˜¸ì¶œ
def x_fetch_latest_tweets(user_id: str, since_id: Optional[str] = None, max_results: int = 5):
    params = {
        "max_results": str(max_results),
        "exclude": "retweets,replies",
        "tweet.fields": "created_at,public_metrics,entities",
    }
    if since_id:
        params["since_id"] = since_id
    js = _x_api_get(f"/2/users/{user_id}/tweets", params=params)
    if not js:
        return [], since_id
    data = js.get("data", [])
    data.sort(key=lambda t: t.get("id"), reverse=False)
    new_since = data[-1]["id"] if data else since_id
    return data, new_since

def _format_tweet_text(t):
    txt = t.get("text", "")
    ents = (t.get("entities") or {}).get("urls", []) or []
    for url in ents:
        u = url.get("url"); ex = url.get("expanded_url") or u
        if u:
            txt = txt.replace(u, ex)
    return txt

# ---------------------------
# UI
# ---------------------------
st.sidebar.title("Tesla One-Stop (Free)")
page = st.sidebar.radio("ë©”ë‰´", ["ğŸ“ˆ ì°¨íŠ¸", "ğŸ“° ë‰´ìŠ¤/ì½”ë©˜íŠ¸", "ğŸ¦ ì§€ë¶„ ë³€ë™(13F, ë¬´ë£Œ)", "âš™ï¸ ì˜µì…˜/ì„¤ì •"])

# ---- Chart ----
if page == "ğŸ“ˆ ì°¨íŠ¸":
    st.title("ğŸ“ˆ TSLA ì°¨íŠ¸ (ì•ˆì •í™” ë²„ì „)")
    c1, c2, c3 = st.columns(3)
    with c1:
        period = st.selectbox("ê¸°ê°„", ["1d", "5d", "7d", "1mo", "3mo", "6mo", "1y"], index=0)
    with c2:
        interval = st.selectbox("ë´‰ ê°„ê²©", ["1m", "5m", "15m", "1h", "1d"], index=0)
    with c3:
        if st.button("ìºì‹œ ì´ˆê¸°í™”", help="yfinanceê°€ ë¹ˆ ê°’ì„ ìºì‹œì— ì €ì¥í–ˆì„ ë•Œ ìœ ìš©"):
            st.cache_data.clear()
            st.success("ìºì‹œ ì‚­ì œ ì™„ë£Œ. ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦")

    df, used, note = safe_yf_download(TSLA, period, interval)
    if note:
        st.info(note)
    if used:
        st.caption(f"ì‹¤ì œ ìš”ì²­: period={used[0]}, interval={used[1]}")

    if df.empty:
        st.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬/íšŒì‚¬ë§ ì°¨ë‹¨ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì¡°í•©ì„ ì„ íƒí•´ ë³´ì„¸ìš”.")
    else:
        plot_candles(df, f"TSLA {used[0]}/{used[1]}")
        if "Close" in df.columns and len(df) > 1:
            last, prev = df["Close"].iloc[-1], df["Close"].iloc[-2]
            st.metric("ì§€ì—° í˜„ì¬ê°€(ì†ŒìŠ¤ ìë™)", f"${last:,.2f}", f"{(last-prev):+.2f}")

# ---- News & Comments ----
elif page == "ğŸ“° ë‰´ìŠ¤/ì½”ë©˜íŠ¸":
    st.title("ğŸ“° ë‰´ìŠ¤ & ì½”ë©˜íŠ¸")
    t1, t2, t3 = st.tabs(["ë‰´ìŠ¤ RSS", "ìœ ëª…ì¸ ì½”ë©˜íŠ¸ (ì„ë² ë“œ, ìë™ ìƒˆë¡œê³ ì¹¨)", "ì‹¤ì‹œê°„ í”¼ë“œ (X API)"])

    # RSS
    with t1:
        cols = st.columns(2)
        keys = list(RSS_SOURCES.keys())
        left_keys, right_keys = keys[: (len(keys)+1)//2], keys[(len(keys)+1)//2 :]
        with cols[0]:
            for k in left_keys:
                st.subheader(k)
                for it in fetch_rss(RSS_SOURCES[k], limit=7):
                    st.markdown(f"- **[{it['title']}]({it['link']})**")
                    if it["published"]:
                        st.caption(it["published"])
                st.markdown("---")
        with cols[1]:
            for k in right_keys:
                st.subheader(k)
                for it in fetch_rss(RSS_SOURCES[k], limit=7):
                    st.markdown(f"- **[{it['title']}]({it['link']})**")
                    if it["published"]:
                        st.caption(it["published"])
                st.markdown("---")

    # X ì„ë² ë“œ + ìë™ ìƒˆë¡œê³ ì¹¨
    with t2:
        acct_label = st.selectbox("ê³„ì • ì„ íƒ", list(X_USERNAMES.keys()), key="embed_acct")
        handle = X_USERNAMES[acct_label]
        col_l, col_r = st.columns([1,3])
        with col_l:
            refresh_sec = st.slider("ìƒˆë¡œê³ ì¹¨(ì´ˆ)", 15, 180, 60, step=15, help="íƒ€ì„ë¼ì¸ì„ ì£¼ê¸°ì ìœ¼ë¡œ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤.")
        st_autorefresh(interval=refresh_sec * 1000, key=f"x_refresh_{handle}")
        embed_html = f"""
        <a class="twitter-timeline" href="https://twitter.com/{handle}?ref_src=twsrc%5Etfw">
          Tweets by @{handle}
        </a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        """
        st.components.v1.html(embed_html, height=800, scrolling=True)

    # X API í´ë§ (ì„ íƒ)
    with t3:
        st.caption("ì„ íƒ ê¸°ëŠ¥: `.streamlit/secrets.toml`ì— X_BEARER_TOKEN ì„¤ì • í•„ìš”")
        if not X_BEARER:
            st.info("X_BEARER_TOKENì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì‹œí¬ë¦¿ì— í† í°ì„ ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            acct_username = st.selectbox("ê³„ì • ì„ íƒ", list(X_USERNAMES.values()), key="api_acct")
            # user_id: í•˜ë“œì½”ë”© ìš°ì„ , ì—†ìœ¼ë©´ API ì¡°íšŒ
            user_id = X_USER_IDS.get(acct_username) or x_get_user_id(acct_username)
            if not user_id:
                st.error("ìœ ì € IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                refresh_sec = st.slider("í´ë§ ì£¼ê¸°(ì´ˆ)", 20, 120, 45, step=5)
                st_autorefresh(interval=refresh_sec*1000, key=f"poll_{acct_username}")

                key_sid = f"since_{acct_username}"
                since_id = st.session_state.get(key_sid)

                tweets, new_since = x_fetch_latest_tweets(user_id, since_id=since_id, max_results=5)
                if new_since and new_since != since_id:
                    st.session_state[key_sid] = new_since

                if not tweets:
                    st.write("ìƒˆ íŠ¸ìœ—ì´ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    for t in reversed(tweets):  # ìµœì‹ ì´ ìœ„ë¡œ ì˜¤ë„ë¡
                        created = t.get("created_at", "")[:19].replace("T", " ")
                        text = _format_tweet_text(t)
                        url = f"https://twitter.com/{acct_username}/status/{t['id']}"
                        st.markdown(f"**@{acct_username}** Â· {created}  \n{text}\n\n[ì›ë¬¸ ë³´ê¸°]({url})")
                        st.markdown("---")

# ---- 13F (FREE) ----
elif page == "ğŸ¦ ì§€ë¶„ ë³€ë™(13F, ë¬´ë£Œ)":
    st.title("ğŸ¦ ê¸°ê´€ ë³´ìœ /ë³€ë™ â€” SEC 13F (ì™„ì „ ë¬´ë£Œ)")
    st.caption("ë¶„ê¸° ê³µì‹œ(13F-HR) ì›ë¬¸ì„ SEC EDGARì—ì„œ íŒŒì‹±, ìµœì‹ /ì§ì „ ë¶„ê¸°ë¥¼ ë¹„êµí•´ TSLA ë³´ìœ  ë³€í™” ê³„ì‚°")

    with st.expander("ëŒ€ìƒ ê¸°ê´€(í¸ì§‘ ê°€ëŠ¥)"):
        edit_df = pd.DataFrame([{"ê¸°ê´€/í€ë“œ": k, "CIK": v} for k, v in DEFAULT_CIKS.items()])
        edited = st.data_editor(edit_df, num_rows="dynamic", key="mgr_edit")
        managers = {row["ê¸°ê´€/í€ë“œ"]: str(row["CIK"]) for _, row in edited.iterrows()
                    if row.get("ê¸°ê´€/í€ë“œ") and row.get("CIK")}
        st.caption("ì¶”ê°€/ì‚­ì œ í›„ ì•„ë˜ í‘œëŠ” ìƒˆë¡œê³ ì¹¨ ì‹œ ë°˜ì˜ë©ë‹ˆë‹¤.")

    refresh = st.button("13F ìƒˆë¡œ ê³ ì¹¨", type="primary")
    if refresh:
        st.cache_data.clear()

    with st.spinner("SECì—ì„œ 13F ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        df = build_13f_table(managers)

    if df.empty:
        st.warning("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. CIKê°€ ë§ëŠ”ì§€ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
    else:
        fmt = df.copy()
        if "ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)" in fmt:
            fmt["ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)"] = fmt["ë³´ìœ ì£¼ìˆ˜(ìµœê·¼)"].map(lambda x: f"{x:,}" if pd.notna(x) else "")
        if "í‰ê°€ì•¡(USD, ìµœê·¼)" in fmt:
            fmt["í‰ê°€ì•¡(USD, ìµœê·¼)"] = fmt["í‰ê°€ì•¡(USD, ìµœê·¼)"].map(lambda x: f"${x:,}" if pd.notna(x) else "")
        if "ë³´ìœ ì£¼ìˆ˜ ì¦ê°(qoq)" in fmt:
            fmt["ë³´ìœ ì£¼ìˆ˜ ì¦ê°(qoq)"] = fmt["ë³´ìœ ì£¼ìˆ˜ ì¦ê°(qoq)"].map(lambda x: f"{x:+,}" if pd.notna(x) else "")
        st.dataframe(fmt, use_container_width=True)

    st.info("ì°¸ê³ : 13FëŠ” **ë¶„ê¸° ë‹¨ìœ„** ê³µê°œì´ë©°, ì¼ì¤‘Â·ì¼ì¼ ë³€ë™ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ---- Settings ----
elif page == "âš™ï¸ ì˜µì…˜/ì„¤ì •":
    st.title("âš™ï¸ ì˜µì…˜/ì„¤ì •")
    st.markdown("""
    - ë³¸ ì•±ì€ **ë¬´ë£Œ ì†ŒìŠ¤**(Yahoo Finance, RSS, SEC EDGAR)ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ë” ë‚˜ì€ ì‹¤ì‹œê°„ ì‹œì„¸ê°€ í•„ìš”í•˜ë©´ Alpha Vantage(ë¬´ë£Œ í‚¤), Polygon/IEX/Finnhub(ìœ ë£Œ/ë¬´ë£Œ í˜¼í•©)ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.
    - SEC ìš”ì²­ì€ User-Agentì— **ì—°ë½ ê°€ëŠ¥í•œ ì´ë©”ì¼** í‘œê¸°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. `your-email@example.com`ì„ ë³¸ì¸ ì£¼ì†Œë¡œ ë°”ê¾¸ì„¸ìš”.
    - X API í´ë§ì€ ì„ íƒ ê¸°ëŠ¥ì…ë‹ˆë‹¤. `.streamlit/secrets.toml`ì— `X_BEARER_TOKEN`ì„ ë„£ìœ¼ë©´ í™œì„±í™”ë©ë‹ˆë‹¤.
    """)
